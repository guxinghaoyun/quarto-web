{"title":"三维重建","markdown":{"headingText":"三维重建","containsRefs":false,"markdown":"\n## 引言：从图像到三维世界的重建\n\n三维重建是计算机视觉的终极目标之一：从二维图像中恢复完整的三维场景结构。这一技术让计算机能够理解真实世界的几何形状、空间布局和物体关系，为虚拟现实、数字文化遗产保护、建筑测量等应用提供了基础支撑。\n\n传统的三维重建方法主要基于多视图几何，通过分析多张图像间的几何关系来恢复三维结构。运动恢复结构（Structure from Motion, SfM）是其中的代表性方法，它能够从无序的图像集合中同时估计相机运动轨迹和场景的三维结构。\n\n现代三维重建技术则融合了深度传感器和神经网络方法。RGB-D重建利用深度相机提供的深度信息，实现实时的三维场景重建；神经辐射场（NeRF）等深度学习方法则能够从稀疏视图中生成高质量的三维表示。这些技术的发展使得三维重建从实验室走向了实际应用。\n\n## 核心概念\n\n**运动恢复结构（SfM）**是传统三维重建的核心方法。其基本思想是：如果我们知道多张图像中特征点的对应关系，就可以通过三角测量恢复这些点的三维坐标，同时估计拍摄这些图像时的相机位置和姿态。SfM的优势在于只需要普通相机即可实现三维重建，但需要场景具有丰富的纹理特征。\n\n**RGB-D重建**利用深度相机（如Kinect、RealSense）提供的彩色图像和深度图像进行三维重建。深度信息的直接获取大大简化了重建过程，使得实时重建成为可能。TSDF（Truncated Signed Distance Function）融合是RGB-D重建的核心技术，它将多帧深度数据融合到统一的体素网格中。\n\n```{mermaid}\ngraph TD\n    subgraph 传统SfM重建\n        A[\"多视图图像\"]\n        B[\"特征提取与匹配\"]\n        C[\"相机姿态估计\"]\n        D[\"三角测量\"]\n        E[\"束调整优化\"]\n    end\n    \n    subgraph RGB-D重建\n        F[\"RGB-D图像序列\"]\n        G[\"相机跟踪\"]\n        H[\"深度图配准\"]\n        I[\"TSDF融合\"]\n        J[\"网格提取\"]\n    end\n    \n    subgraph 神经网络重建\n        K[\"稀疏视图\"]\n        L[\"神经辐射场\"]\n        M[\"体渲染\"]\n        N[\"新视图合成\"]\n        O[\"几何提取\"]\n    end\n    \n    A --> B --> C --> D --> E\n    F --> G --> H --> I --> J\n    K --> L --> M --> N --> O\n    \n    classDef sfmNode fill:#42a5f5,stroke:#1565c0,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    classDef rgbdNode fill:#66bb6a,stroke:#2e7d32,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    classDef neuralNode fill:#ba68c8,stroke:#7b1fa2,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    \n    classDef sfmSubgraph fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1,font-weight:bold\n    classDef rgbdSubgraph fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20,font-weight:bold\n    classDef neuralSubgraph fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c,font-weight:bold\n    \n    class A,B,C,D,E sfmNode\n    class F,G,H,I,J rgbdNode\n    class K,L,M,N,O neuralNode\n    \n    class 传统SfM重建 sfmSubgraph\n    class RGB-D重建 rgbdSubgraph\n    class 神经网络重建 neuralSubgraph\n    \n    linkStyle 0,1,2,3 stroke:#1565c0,stroke-width:2px\n    linkStyle 4,5,6,7 stroke:#2e7d32,stroke-width:2px\n    linkStyle 8,9,10,11 stroke:#7b1fa2,stroke-width:2px\n```\n*图11.12：三种主要三维重建方法的技术流程对比*\n\n**神经辐射场（NeRF）**代表了三维重建的最新发展方向。它使用多层感知机（MLP）来表示三维场景，将空间坐标和视角方向映射为颜色和密度值。通过体渲染技术，NeRF能够生成任意视角的高质量图像，并隐式地表示场景的三维几何结构。\n\n**TSDF融合**是RGB-D重建中的关键技术。TSDF将三维空间划分为规则的体素网格，每个体素存储到最近表面的有符号距离。通过融合多帧深度数据，TSDF能够处理噪声和遮挡，生成平滑的三维表面。\n\n```{mermaid}\ngraph LR\n    subgraph TSDF融合过程\n        A[\"深度图1<br/>Frame t\"]\n        B[\"深度图2<br/>Frame t+1\"]\n        C[\"深度图N<br/>Frame t+n\"]\n    end\n    \n    subgraph 体素网格\n        D[\"TSDF值<br/>有符号距离\"]\n        E[\"权重值<br/>置信度\"]\n        F[\"颜色值<br/>RGB信息\"]\n    end\n    \n    subgraph 表面重建\n        G[\"Marching Cubes<br/>等值面提取\"]\n        H[\"三角网格<br/>Mesh\"]\n    end\n    \n    A --> D\n    B --> D\n    C --> D\n    A --> E\n    B --> E\n    C --> E\n    A --> F\n    B --> F\n    C --> F\n    \n    D --> G\n    E --> G\n    F --> G\n    G --> H\n    \n    classDef depthNode fill:#ffb74d,stroke:#e65100,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    classDef voxelNode fill:#ef5350,stroke:#c62828,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    classDef meshNode fill:#4caf50,stroke:#2e7d32,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    \n    classDef depthSubgraph fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#bf360c,font-weight:bold\n    classDef voxelSubgraph fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#b71c1c,font-weight:bold\n    classDef meshSubgraph fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20,font-weight:bold\n    \n    class A,B,C depthNode\n    class D,E,F voxelNode\n    class G,H meshNode\n    \n    class TSDF融合过程 depthSubgraph\n    class 体素网格 voxelSubgraph\n    class 表面重建 meshSubgraph\n    \n    linkStyle 0,1,2,3,4,5,6,7,8,9,10,11,12 stroke-width:1.5px\n```\n*图11.13：TSDF融合的数据流程和体素网格表示*\n\n## 理论基础：从多视图几何到神经隐式表示\n\n三维重建的理论基础涵盖了传统几何方法和现代神经网络方法，下面我们分别介绍这些方法的核心理论。\n\n### 运动恢复结构（SfM）的理论基础\n\nSfM的理论基础是多视图几何和投影模型。对于空间中的点$\\mathbf{X} = (X, Y, Z, 1)^T$，其在图像$i$中的投影点$\\mathbf{x}_i = (u_i, v_i, 1)^T$满足：\n\n$$\\lambda_i \\mathbf{x}_i = \\mathbf{P}_i \\mathbf{X} = \\mathbf{K}_i [\\mathbf{R}_i | \\mathbf{t}_i] \\mathbf{X}$$\n\n其中，$\\mathbf{P}_i$是投影矩阵，$\\mathbf{K}_i$是内参矩阵，$\\mathbf{R}_i$和$\\mathbf{t}_i$分别是旋转矩阵和平移向量，$\\lambda_i$是尺度因子。\n\nSfM的核心问题是：已知多张图像中的对应点$\\{\\mathbf{x}_i\\}$，如何恢复相机参数$\\{\\mathbf{P}_i\\}$和三维点$\\mathbf{X}$？这个问题可以通过以下步骤解决：\n\n**1. 特征匹配与基础矩阵估计**\n\n对于两张图像，我们首先提取特征点（如SIFT、ORB）并建立匹配。然后估计基础矩阵$\\mathbf{F}$，它满足对极约束：\n\n$$\\mathbf{x}_2^T \\mathbf{F} \\mathbf{x}_1 = 0$$\n\n基础矩阵可以通过8点法或RANSAC算法估计。\n\n**2. 相机姿态估计**\n\n从基础矩阵$\\mathbf{F}$可以分解出本质矩阵$\\mathbf{E}$：\n\n$$\\mathbf{E} = \\mathbf{K}_2^T \\mathbf{F} \\mathbf{K}_1$$\n\n进一步分解本质矩阵可得到相对旋转$\\mathbf{R}$和平移$\\mathbf{t}$：\n\n$$\\mathbf{E} = [\\mathbf{t}]_{\\times} \\mathbf{R}$$\n\n其中$[\\mathbf{t}]_{\\times}$是$\\mathbf{t}$的反对称矩阵。\n\n**3. 三角测量**\n\n已知两个相机的投影矩阵$\\mathbf{P}_1$和$\\mathbf{P}_2$，以及对应点$\\mathbf{x}_1$和$\\mathbf{x}_2$，可以通过三角测量恢复三维点$\\mathbf{X}$。这可以表示为一个线性方程组：\n\n$$\n\\begin{bmatrix}\n\\mathbf{x}_1 \\times \\mathbf{P}_1 \\\\\n\\mathbf{x}_2 \\times \\mathbf{P}_2\n\\end{bmatrix} \\mathbf{X} = \\mathbf{0}\n$$\n\n通过SVD求解这个方程组的最小二乘解。\n\n**4. 束调整优化**\n\n最后，通过束调整（Bundle Adjustment）优化相机参数和三维点坐标，最小化重投影误差：\n\n$$\\min_{\\{\\mathbf{P}_i\\}, \\{\\mathbf{X}_j\\}} \\sum_{i,j} d(\\mathbf{x}_{ij}, \\mathbf{P}_i \\mathbf{X}_j)^2$$\n\n其中$d(\\cdot, \\cdot)$是欧氏距离，$\\mathbf{x}_{ij}$是第$j$个三维点在第$i$个相机中的观测。\n\n### TSDF融合的理论基础\n\nTSDF（Truncated Signed Distance Function）是一种隐式表面表示方法，它将三维空间划分为规则的体素网格，每个体素存储到最近表面的有符号距离。\n\n对于空间中的点$\\mathbf{p} = (x, y, z)$，其TSDF值定义为：\n\n$$TSDF(\\mathbf{p}) = \\begin{cases}\n\\min(1, \\frac{d(\\mathbf{p})}{t}) & \\text{if } d(\\mathbf{p}) \\geq 0 \\\\\n\\max(-1, \\frac{d(\\mathbf{p})}{t}) & \\text{if } d(\\mathbf{p}) < 0\n\\end{cases}$$\n\n其中，$d(\\mathbf{p})$是点$\\mathbf{p}$到最近表面的有符号距离，$t$是截断距离。正值表示点在表面外部，负值表示点在表面内部，零值表示点在表面上。\n\nTSDF融合的核心是将多帧深度图融合到统一的TSDF体素网格中。对于第$k$帧深度图，每个体素的TSDF值和权重更新如下：\n\n$$TSDF_k(\\mathbf{p}) = \\frac{W_{k-1}(\\mathbf{p}) \\cdot TSDF_{k-1}(\\mathbf{p}) + w_k(\\mathbf{p}) \\cdot TSDF_k'(\\mathbf{p})}{W_{k-1}(\\mathbf{p}) + w_k(\\mathbf{p})}$$\n\n$$W_k(\\mathbf{p}) = W_{k-1}(\\mathbf{p}) + w_k(\\mathbf{p})$$\n\n其中，$TSDF_k'(\\mathbf{p})$是从当前深度图计算的TSDF值，$w_k(\\mathbf{p})$是当前测量的权重。\n\n最后，通过Marching Cubes算法从TSDF体素网格中提取等值面（零值面），得到三维表面的三角网格表示。\n\n### 神经辐射场（NeRF）的理论基础\n\nNeRF是一种基于神经网络的隐式场景表示方法。它使用多层感知机（MLP）来表示三维场景，将空间坐标$\\mathbf{x} = (x, y, z)$和视角方向$\\mathbf{d} = (\\theta, \\phi)$映射为颜色$\\mathbf{c} = (r, g, b)$和密度$\\sigma$：\n\n$$F_\\Theta: (\\mathbf{x}, \\mathbf{d}) \\rightarrow (\\mathbf{c}, \\sigma)$$\n\n其中，$F_\\Theta$是参数为$\\Theta$的神经网络。\n\n给定一条从相机中心出发的光线$\\mathbf{r}(t) = \\mathbf{o} + t\\mathbf{d}$，NeRF通过体渲染方程计算该光线上的颜色：\n\n$$C(\\mathbf{r}) = \\int_{t_n}^{t_f} T(t) \\sigma(\\mathbf{r}(t)) \\mathbf{c}(\\mathbf{r}(t), \\mathbf{d}) dt$$\n\n其中，$T(t) = \\exp(-\\int_{t_n}^{t} \\sigma(\\mathbf{r}(s)) ds)$是累积透射率，表示光线从$t_n$到$t$的透明度。\n\n在实践中，这个积分通过离散采样近似计算：\n\n$$\\hat{C}(\\mathbf{r}) = \\sum_{i=1}^{N} T_i (1 - \\exp(-\\sigma_i \\delta_i)) \\mathbf{c}_i$$\n\n其中，$T_i = \\exp(-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j)$，$\\delta_i$是相邻采样点之间的距离。\n\nNeRF通过最小化渲染图像与真实图像之间的差异来优化网络参数：\n\n$$\\mathcal{L} = \\sum_{\\mathbf{r} \\in \\mathcal{R}} \\|\\hat{C}(\\mathbf{r}) - C_{gt}(\\mathbf{r})\\|_2^2$$\n\n其中，$\\mathcal{R}$是训练集中的所有光线，$C_{gt}(\\mathbf{r})$是光线$\\mathbf{r}$对应的真实颜色。\n\n## 算法实现\n\n下面我们分别介绍SfM、TSDF融合和NeRF的核心算法实现。\n\n### SfM的核心算法实现\n\nSfM的实现通常基于特征匹配和几何优化。以下是使用OpenCV实现的SfM核心步骤：\n\n```python\nimport cv2\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef structure_from_motion(images):\n    \"\"\"SfM核心算法实现\"\"\"\n    # 1. 特征提取与匹配\n    features = extract_features(images)\n    matches = match_features(features)\n\n    # 2. 初始化重建（从两视图开始）\n    K = estimate_camera_intrinsics()  # 假设已知或通过标定获得\n    E, mask = cv2.findEssentialMat(matches[0], matches[1], K)\n    _, R, t, _ = cv2.recoverPose(E, matches[0], matches[1], K)\n\n    # 初始相机矩阵\n    P1 = np.hstack((np.eye(3), np.zeros((3, 1))))\n    P2 = np.hstack((R, t))\n\n    # 3. 三角测量初始点云\n    points_3d = triangulate_points(matches[0], matches[1], P1, P2, K)\n\n    # 4. 增量式SfM\n    for i in range(2, len(images)):\n        # 2D-3D对应关系\n        points_2d = find_2d_3d_correspondences(features[i], points_3d)\n\n        # PnP求解相机位姿\n        _, rvec, tvec, inliers = cv2.solvePnPRansac(\n            points_3d, points_2d, K, None)\n        R_new = cv2.Rodrigues(rvec)[0]\n        t_new = tvec\n\n        # 更新点云\n        new_matches = find_new_matches(features[i-1], features[i])\n        new_points_3d = triangulate_points(\n            new_matches[0], new_matches[1],\n            P1, np.hstack((R_new, t_new)), K)\n        points_3d = np.vstack((points_3d, new_points_3d))\n\n        # 5. 束调整优化\n        camera_params, points_3d = bundle_adjustment(\n            camera_params, points_3d, observations)\n\n    return camera_params, points_3d\n\ndef bundle_adjustment(camera_params, points_3d, observations):\n    \"\"\"束调整核心实现\"\"\"\n    # 定义重投影误差函数\n    def reprojection_error(params, n_cameras, n_points, camera_indices,\n                          point_indices, observations):\n        camera_params = params[:n_cameras * 6].reshape((n_cameras, 6))\n        points_3d = params[n_cameras * 6:].reshape((n_points, 3))\n\n        projected = project(points_3d[point_indices], camera_params[camera_indices])\n        return (projected - observations).ravel()\n\n    # 参数打包\n    params = np.hstack((camera_params.ravel(), points_3d.ravel()))\n\n    # 最小化重投影误差\n    result = least_squares(\n        reprojection_error, params,\n        args=(n_cameras, n_points, camera_indices, point_indices, observations),\n        method='trf', ftol=1e-4, xtol=1e-4, gtol=1e-4)\n\n    # 参数解包\n    params = result.x\n    camera_params = params[:n_cameras * 6].reshape((n_cameras, 6))\n    points_3d = params[n_cameras * 6:].reshape((n_points, 3))\n\n    return camera_params, points_3d\n```\n\n### TSDF融合的核心算法实现\n\nTSDF融合算法的核心是将深度图转换为TSDF表示，并融合多帧数据：\n\n```python\nimport numpy as np\n\nclass TSDFVolume:\n    \"\"\"TSDF体素网格表示\"\"\"\n    def __init__(self, vol_bounds, voxel_size, trunc_margin):\n        # 初始化体素网格\n        self.voxel_size = voxel_size\n        self.trunc_margin = trunc_margin\n\n        # 计算体素网格尺寸\n        vol_dim = np.ceil((vol_bounds[:, 1] - vol_bounds[:, 0]) / voxel_size).astype(int)\n        self.vol_bounds = vol_bounds\n        self.vol_dim = vol_dim\n\n        # 初始化TSDF值和权重\n        self.voxel_grid_tsdf = np.ones(vol_dim) * 1.0\n        self.voxel_grid_weight = np.zeros(vol_dim)\n\n        # 计算体素中心坐标\n        self._compute_voxel_centers()\n\n    def _compute_voxel_centers(self):\n        \"\"\"计算体素中心坐标\"\"\"\n        # 创建体素中心坐标网格\n        xv, yv, zv = np.meshgrid(\n            np.arange(0, self.vol_dim[0]),\n            np.arange(0, self.vol_dim[1]),\n            np.arange(0, self.vol_dim[2]))\n\n        # 转换为世界坐标\n        self.voxel_centers = np.stack([xv, yv, zv], axis=-1) * self.voxel_size + self.vol_bounds[:, 0]\n\n    def integrate(self, depth_img, K, pose):\n        \"\"\"将深度图融合到TSDF体素网格中\"\"\"\n        # 将体素中心投影到深度图\n        cam_pts = self.voxel_centers.reshape(-1, 3)\n        cam_pts = np.matmul(cam_pts - pose[:3, 3], pose[:3, :3].T)\n\n        # 投影到图像平面\n        pix_x = np.round(cam_pts[:, 0] * K[0, 0] / cam_pts[:, 2] + K[0, 2]).astype(int)\n        pix_y = np.round(cam_pts[:, 1] * K[1, 1] / cam_pts[:, 2] + K[1, 2]).astype(int)\n\n        # 检查像素是否在图像范围内\n        valid_pix = (pix_x >= 0) & (pix_x < depth_img.shape[1]) & \\\n                    (pix_y >= 0) & (pix_y < depth_img.shape[0]) & \\\n                    (cam_pts[:, 2] > 0)\n\n        # 获取有效像素的深度值\n        depth_values = np.zeros(pix_x.shape)\n        depth_values[valid_pix] = depth_img[pix_y[valid_pix], pix_x[valid_pix]]\n\n        # 计算TSDF值\n        dist = depth_values - cam_pts[:, 2]\n        tsdf_values = np.minimum(1.0, dist / self.trunc_margin)\n        tsdf_values = np.maximum(-1.0, tsdf_values)\n\n        # 计算权重\n        weights = (depth_values > 0).astype(float)\n\n        # 更新TSDF值和权重\n        tsdf_vol_new = self.voxel_grid_tsdf.reshape(-1)\n        weight_vol_new = self.voxel_grid_weight.reshape(-1)\n\n        # 融合TSDF值\n        mask = valid_pix & (depth_values > 0) & (dist > -self.trunc_margin)\n        tsdf_vol_new[mask] = (weight_vol_new[mask] * tsdf_vol_new[mask] + weights[mask] * tsdf_values[mask]) / \\\n                             (weight_vol_new[mask] + weights[mask])\n\n        # 更新权重\n        weight_vol_new[mask] += weights[mask]\n\n        # 重塑回原始形状\n        self.voxel_grid_tsdf = tsdf_vol_new.reshape(self.vol_dim)\n        self.voxel_grid_weight = weight_vol_new.reshape(self.vol_dim)\n```\n\n### NeRF的核心算法实现\n\nNeRF使用PyTorch实现，核心是神经网络模型和体渲染算法：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass NeRF(nn.Module):\n    \"\"\"神经辐射场核心网络\"\"\"\n    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, output_ch=4):\n        super(NeRF, self).__init__()\n        self.D = D\n        self.W = W\n        self.input_ch = input_ch\n        self.input_ch_views = input_ch_views\n        self.output_ch = output_ch\n\n        # 位置编码后的输入维度\n        input_ch = self.input_ch\n\n        # 主干网络\n        self.pts_linears = nn.ModuleList(\n            [nn.Linear(input_ch, W)] +\n            [nn.Linear(W, W) for _ in range(D-1)])\n\n        # 密度输出层\n        self.alpha_linear = nn.Linear(W, 1)\n\n        # 视角相关特征\n        self.feature_linear = nn.Linear(W, W)\n        self.views_linears = nn.ModuleList([nn.Linear(W + input_ch_views, W//2)])\n\n        # RGB输出层\n        self.rgb_linear = nn.Linear(W//2, 3)\n\n    def forward(self, x):\n        \"\"\"前向传播\"\"\"\n        # 分离位置和方向输入\n        input_pts, input_views = torch.split(\n            x, [self.input_ch, self.input_ch_views], dim=-1)\n\n        # 处理位置信息\n        h = input_pts\n        for i, l in enumerate(self.pts_linears):\n            h = self.pts_linears[i](h)\n            h = F.relu(h)\n\n        # 密度预测\n        alpha = self.alpha_linear(h)\n\n        # 特征向量\n        feature = self.feature_linear(h)\n\n        # 处理视角信息\n        h = torch.cat([feature, input_views], -1)\n        for i, l in enumerate(self.views_linears):\n            h = self.views_linears[i](h)\n            h = F.relu(h)\n\n        # RGB预测\n        rgb = self.rgb_linear(h)\n        rgb = torch.sigmoid(rgb)\n\n        # 输出RGB和密度\n        outputs = torch.cat([rgb, alpha], -1)\n        return outputs\n\ndef render_rays(model, rays_o, rays_d, near, far, N_samples):\n    \"\"\"体渲染核心算法\"\"\"\n    # 在光线上采样点\n    t_vals = torch.linspace(0., 1., steps=N_samples)\n    z_vals = near * (1.-t_vals) + far * t_vals\n\n    # 扰动采样点位置（分层采样）\n    z_vals = z_vals.expand([rays_o.shape[0], N_samples])\n    mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n    upper = torch.cat([mids, z_vals[...,-1:]], -1)\n    lower = torch.cat([z_vals[...,:1], mids], -1)\n    t_rand = torch.rand(z_vals.shape)\n    z_vals = lower + (upper - lower) * t_rand\n\n    # 计算采样点的3D坐标\n    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n\n    # 查询网络\n    raw = model(pts)\n\n    # 体渲染积分\n    dists = z_vals[...,1:] - z_vals[...,:-1]\n    dists = torch.cat([dists, torch.ones_like(dists[...,:1]) * 1e10], -1)\n\n    # 计算alpha值\n    alpha = 1.0 - torch.exp(-raw[...,3] * dists)\n\n    # 计算权重\n    weights = alpha * torch.cumprod(\n        torch.cat([torch.ones_like(alpha[...,:1]), 1.-alpha[...,:-1]], -1), -1)\n\n    # 计算颜色\n    rgb = torch.sum(weights[...,None] * raw[...,:3], -2)\n\n    # 计算深度\n    depth = torch.sum(weights * z_vals, -1)\n\n    return rgb, depth, weights\n```\n## 重建质量评估\n\n三维重建算法的效果可以从重建精度、计算效率和应用场景适应性等多个维度进行评估。\n\n### 方法性能对比\n\n```{mermaid}\ngraph TD\n    subgraph 传统SfM方法\n        A[\"COLMAP<br/>精度: 高<br/>速度: 慢<br/>内存: 中\"]\n        B[\"OpenMVG<br/>精度: 中<br/>速度: 中<br/>内存: 低\"]\n        C[\"VisualSFM<br/>精度: 中<br/>速度: 快<br/>内存: 低\"]\n    end\n\n    subgraph RGB-D重建方法\n        D[\"KinectFusion<br/>精度: 中<br/>速度: 快<br/>内存: 高\"]\n        E[\"ElasticFusion<br/>精度: 高<br/>速度: 中<br/>内存: 高\"]\n        F[\"BundleFusion<br/>精度: 很高<br/>速度: 慢<br/>内存: 很高\"]\n    end\n\n    subgraph 神经网络方法\n        G[\"NeRF<br/>精度: 很高<br/>速度: 很慢<br/>内存: 中\"]\n        H[\"Instant-NGP<br/>精度: 高<br/>速度: 快<br/>内存: 低\"]\n        I[\"DVGO<br/>精度: 高<br/>速度: 中<br/>内存: 高\"]\n    end\n\n    subgraph 评估指标\n        J[\"重建精度<br/>几何误差\"]\n        K[\"纹理质量<br/>视觉效果\"]\n        L[\"计算效率<br/>时间复杂度\"]\n    end\n\n    A --> J\n    B --> J\n    C --> J\n    D --> J\n    E --> J\n    F --> J\n    G --> J\n    H --> J\n    I --> J\n\n    J --> M[\"SfM: mm级<br/>RGB-D: cm级<br/>NeRF: sub-mm级\"]\n    K --> N[\"SfM: 中等<br/>RGB-D: 低<br/>NeRF: 很高\"]\n    L --> O[\"SfM: 小时级<br/>RGB-D: 实时<br/>NeRF: 天级\"]\n\n    classDef sfmNode fill:#42a5f5,stroke:#1565c0,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef rgbdNode fill:#66bb6a,stroke:#2e7d32,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef neuralNode fill:#ba68c8,stroke:#7b1fa2,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef metricNode fill:#ffb74d,stroke:#e65100,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef resultNode fill:#ef5350,stroke:#c62828,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n\n    classDef sfmSubgraph fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1,font-weight:bold\n    classDef rgbdSubgraph fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20,font-weight:bold\n    classDef neuralSubgraph fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c,font-weight:bold\n\n    class A,B,C sfmNode\n    class D,E,F rgbdNode\n    class G,H,I neuralNode\n    class J,K,L metricNode\n    class M,N,O resultNode\n\n    class 传统SfM方法 sfmSubgraph\n    class RGB-D重建方法 rgbdSubgraph\n    class 神经网络方法 neuralSubgraph\n\n    linkStyle 0,1,2,3,4,5,6,7,8 stroke:#1565c0,stroke-width:1.5px\n    linkStyle 9,10,11 stroke:#4caf50,stroke-width:1.5px\n```\n*图11.14：不同三维重建方法的性能对比分析*\n\n### 应用场景适应性\n\n```{mermaid}\ngraph LR\n    subgraph 室外大场景\n        A[\"文化遗产保护<br/>高精度要求\"]\n        B[\"城市建模<br/>大规模重建\"]\n        C[\"地形测绘<br/>几何精度优先\"]\n    end\n\n    subgraph 室内小场景\n        D[\"AR/VR应用<br/>实时性要求\"]\n        E[\"机器人导航<br/>动态更新\"]\n        F[\"医疗重建<br/>高精度要求\"]\n    end\n\n    subgraph 特殊场景\n        G[\"弱纹理环境<br/>几何约束\"]\n        H[\"动态场景<br/>时序一致性\"]\n        I[\"稀疏视图<br/>先验知识\"]\n    end\n\n    A --> J[\"SfM + 摄影测量<br/>精度: 很高<br/>成本: 低\"]\n    B --> K[\"SfM + 航拍<br/>精度: 高<br/>成本: 中\"]\n    C --> J\n\n    D --> L[\"RGB-D实时重建<br/>精度: 中<br/>成本: 中\"]\n    E --> L\n    F --> M[\"高精度RGB-D<br/>精度: 很高<br/>成本: 高\"]\n\n    G --> N[\"几何约束SfM<br/>精度: 中<br/>成本: 低\"]\n    H --> O[\"动态NeRF<br/>精度: 高<br/>成本: 很高\"]\n    I --> P[\"NeRF + 先验<br/>精度: 很高<br/>成本: 高\"]\n\n    classDef outdoorNode fill:#4db6ac,stroke:#00796b,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef indoorNode fill:#ba68c8,stroke:#7b1fa2,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef specialNode fill:#ffb74d,stroke:#e65100,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef solutionNode fill:#90caf9,stroke:#1976d2,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n\n    classDef outdoorSubgraph fill:#e0f2f1,stroke:#00796b,stroke-width:2px,color:#004d40,font-weight:bold\n    classDef indoorSubgraph fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c,font-weight:bold\n    classDef specialSubgraph fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#bf360c,font-weight:bold\n\n    class A,B,C outdoorNode\n    class D,E,F indoorNode\n    class G,H,I specialNode\n    class J,K,L,M,N,O,P solutionNode\n\n    class 室外大场景 outdoorSubgraph\n    class 室内小场景 indoorSubgraph\n    class 特殊场景 specialSubgraph\n\n    linkStyle 0,1,2,3,4,5,6,7,8 stroke-width:1.5px\n```\n*图11.15：三维重建方法在不同应用场景中的适应性*\n\n### 技术发展趋势\n\n``` {mermaid}\ngraph TD\n    subgraph 传统方法演进\n        A[早期SfM<br/>2000-2010]\n        B[增量式SfM<br/>2010-2015]\n        C[全局SfM<br/>2015-2020]\n    end\n\n    subgraph 深度传感器融合\n        D[KinectFusion<br/>2011]\n        E[ElasticFusion<br/>2015]\n        F[BundleFusion<br/>2017]\n    end\n\n    subgraph 神经网络革命\n        G[NeRF<br/>2020]\n        H[Instant-NGP<br/>2022]\n        I[3D Gaussian<br/>2023]\n    end\n\n    subgraph 未来发展方向\n        J[实时神经重建]\n        K[多模态融合]\n        L[语义感知重建]\n        M[自监督学习]\n    end\n\n    A --> B --> C\n    D --> E --> F\n    G --> H --> I\n\n    C --> J\n    F --> K\n    I --> L\n    I --> M\n\n    classDef tradNode fill:#64b5f6,stroke:#1565c0,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef rgbdNode fill:#66bb6a,stroke:#2e7d32,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef neuralNode fill:#ba68c8,stroke:#7b1fa2,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef futureNode fill:#ffb74d,stroke:#e65100,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n\n    class A,B,C tradNode\n    class D,E,F rgbdNode\n    class G,H,I neuralNode\n    class J,K,L,M futureNode\n```\n\n*图11.16：三维重建技术的发展历程和未来趋势*\n\n## 小结\n\n三维重建是计算机视觉的核心技术之一，经历了从传统几何方法到现代神经网络方法的重要演进。传统SfM方法基于多视图几何，能够从普通图像中恢复三维结构，但需要丰富的纹理特征；RGB-D重建利用深度传感器，实现了实时重建，但受限于传感器范围；神经辐射场等深度学习方法则能够生成高质量的三维表示，但计算成本较高。\n\n本节的核心贡献在于：**理论层面**，系统阐述了从多视图几何到神经隐式表示的理论基础；**技术层面**，对比了SfM、TSDF融合和NeRF的核心算法差异；**应用层面**，分析了不同方法在各类场景中的适应性和发展趋势。\n\n三维重建技术与前面章节的相机标定和立体匹配紧密相连：相机标定提供了准确的几何参数，立体匹配提供了深度信息，而三维重建则将这些信息整合为完整的三维模型。随着神经网络技术的发展，三维重建正朝着更高质量、更高效率、更强泛化能力的方向发展，在数字孪生、元宇宙等新兴应用中发挥着越来越重要的作用。\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"atom-one","output-file":"11.3_三维重建.html"},"language":{"toc-title-document":"目录","toc-title-website":"该页面内容","related-formats-title":"其他格式","related-notebooks-title":"笔记本","source-notebooks-prefix":"资源","other-links-title":"其他链接","code-links-title":"代码链接","launch-dev-container-title":"启动 Dev Container","launch-binder-title":"启动 Binder","article-notebook-label":"文章笔记本","notebook-preview-download":"下载笔记本","notebook-preview-download-src":"下载源代码","notebook-preview-back":"返回文章","manuscript-meca-bundle":"MECA 存档","section-title-abstract":"摘要","section-title-appendices":"附录","section-title-footnotes":"脚注","section-title-references":"参考文献","section-title-reuse":"二次使用","section-title-copyright":"版权","section-title-citation":"引用格式","appendix-attribution-cite-as":"请按如下格式引用：","appendix-attribution-bibtex":"BibTeX","appendix-view-license":"查看许可协议","title-block-author-single":"作者","title-block-author-plural":"作者","title-block-affiliation-single":"单位","title-block-affiliation-plural":"单位","title-block-published":"发布于","title-block-modified":"修改于","title-block-keywords":"关键词","callout-tip-title":"提示","callout-note-title":"注记","callout-warning-title":"警告","callout-important-title":"重要","callout-caution-title":"注意","code-summary":"代码","code-tools-menu-caption":"代码","code-tools-show-all-code":"显示所有代码","code-tools-hide-all-code":"隐藏所有代码","code-tools-view-source":"查看源代码","code-tools-source-code":"源代码","tools-share":"Share","tools-download":"Download","code-line":"行","code-lines":"行","copy-button-tooltip":"复制到剪贴板","copy-button-tooltip-success":"已复制","repo-action-links-edit":"编辑该页面","repo-action-links-source":"查看代码","repo-action-links-issue":"反馈问题","back-to-top":"回到顶部","search-no-results-text":"没有结果","search-matching-documents-text":"匹配的文档","search-copy-link-title":"复制搜索链接","search-hide-matches-text":"隐藏其它匹配结果","search-more-match-text":"更多匹配结果","search-more-matches-text":"更多匹配结果","search-clear-button-title":"清除","search-text-placeholder":"","search-detached-cancel-button-title":"取消","search-submit-button-title":"提交","search-label":"搜索","toggle-section":"展开或折叠此栏","toggle-sidebar":"展开或折叠侧边栏导航","toggle-dark-mode":"切换深色模式","toggle-reader-mode":"切换阅读器模式","toggle-navigation":"展开或折叠导航栏","crossref-fig-title":"图","crossref-tbl-title":"表","crossref-lst-title":"列表","crossref-thm-title":"定理","crossref-lem-title":"引理","crossref-cor-title":"推论","crossref-prp-title":"命题","crossref-cnj-title":"猜想","crossref-def-title":"定义","crossref-exm-title":"例","crossref-exr-title":"习题","crossref-ch-prefix":"章节","crossref-apx-prefix":"附录","crossref-sec-prefix":"小节","crossref-eq-prefix":"式","crossref-lof-title":"图索引","crossref-lot-title":"表索引","crossref-lol-title":"列表索引","environment-proof-title":"证","environment-remark-title":"注记","environment-solution-title":"解","listing-page-order-by":"排序方式","listing-page-order-by-default":"默认","listing-page-order-by-date-asc":"日期升序","listing-page-order-by-date-desc":"日期降序","listing-page-order-by-number-desc":"降序","listing-page-order-by-number-asc":"升序","listing-page-field-date":"日期","listing-page-field-title":"标题","listing-page-field-description":"描述","listing-page-field-author":"作者","listing-page-field-filename":"文件名","listing-page-field-filemodified":"修改时间","listing-page-field-subtitle":"副标题","listing-page-field-readingtime":"阅读时间","listing-page-field-wordcount":"字数统计","listing-page-field-categories":"分类","listing-page-minutes-compact":"{0} 分钟","listing-page-category-all":"全部","listing-page-no-matches":"无匹配项","listing-page-words":"{0} 字","listing-page-filter":"筛选","draft":"草稿"},"metadata":{"lang":"zh-CN","fig-responsive":true,"quarto-version":"1.7.32","thesis-title":"现代计算机视觉","author":[{"name":"AI Assistant & You"}],"supervisor":[{"name":"Supervisor Name"}],"degree":"Doctor of Philosophy","degree-year":"2024","declaration-text":"This is my declaration.","acknowledgements-text":"I want to thank...","bibliography":["../references.bib"],"mermaid":{"theme":"default"},"theme":{"light":"flatly","dark":"cyborg"},"mainfont":"WenQuanYi Micro Hei"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"mermaid-format":"png","df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"11.3_三维重建.pdf"},"language":{"toc-title-document":"目录","toc-title-website":"该页面内容","related-formats-title":"其他格式","related-notebooks-title":"笔记本","source-notebooks-prefix":"资源","other-links-title":"其他链接","code-links-title":"代码链接","launch-dev-container-title":"启动 Dev Container","launch-binder-title":"启动 Binder","article-notebook-label":"文章笔记本","notebook-preview-download":"下载笔记本","notebook-preview-download-src":"下载源代码","notebook-preview-back":"返回文章","manuscript-meca-bundle":"MECA 存档","section-title-abstract":"摘要","section-title-appendices":"附录","section-title-footnotes":"脚注","section-title-references":"参考文献","section-title-reuse":"二次使用","section-title-copyright":"版权","section-title-citation":"引用格式","appendix-attribution-cite-as":"请按如下格式引用：","appendix-attribution-bibtex":"BibTeX","appendix-view-license":"查看许可协议","title-block-author-single":"作者","title-block-author-plural":"作者","title-block-affiliation-single":"单位","title-block-affiliation-plural":"单位","title-block-published":"发布于","title-block-modified":"修改于","title-block-keywords":"关键词","callout-tip-title":"提示","callout-note-title":"注记","callout-warning-title":"警告","callout-important-title":"重要","callout-caution-title":"注意","code-summary":"代码","code-tools-menu-caption":"代码","code-tools-show-all-code":"显示所有代码","code-tools-hide-all-code":"隐藏所有代码","code-tools-view-source":"查看源代码","code-tools-source-code":"源代码","tools-share":"Share","tools-download":"Download","code-line":"行","code-lines":"行","copy-button-tooltip":"复制到剪贴板","copy-button-tooltip-success":"已复制","repo-action-links-edit":"编辑该页面","repo-action-links-source":"查看代码","repo-action-links-issue":"反馈问题","back-to-top":"回到顶部","search-no-results-text":"没有结果","search-matching-documents-text":"匹配的文档","search-copy-link-title":"复制搜索链接","search-hide-matches-text":"隐藏其它匹配结果","search-more-match-text":"更多匹配结果","search-more-matches-text":"更多匹配结果","search-clear-button-title":"清除","search-text-placeholder":"","search-detached-cancel-button-title":"取消","search-submit-button-title":"提交","search-label":"搜索","toggle-section":"展开或折叠此栏","toggle-sidebar":"展开或折叠侧边栏导航","toggle-dark-mode":"切换深色模式","toggle-reader-mode":"切换阅读器模式","toggle-navigation":"展开或折叠导航栏","crossref-fig-title":"图","crossref-tbl-title":"表","crossref-lst-title":"列表","crossref-thm-title":"定理","crossref-lem-title":"引理","crossref-cor-title":"推论","crossref-prp-title":"命题","crossref-cnj-title":"猜想","crossref-def-title":"定义","crossref-exm-title":"例","crossref-exr-title":"习题","crossref-ch-prefix":"章节","crossref-apx-prefix":"附录","crossref-sec-prefix":"小节","crossref-eq-prefix":"式","crossref-lof-title":"图索引","crossref-lot-title":"表索引","crossref-lol-title":"列表索引","environment-proof-title":"证","environment-remark-title":"注记","environment-solution-title":"解","listing-page-order-by":"排序方式","listing-page-order-by-default":"默认","listing-page-order-by-date-asc":"日期升序","listing-page-order-by-date-desc":"日期降序","listing-page-order-by-number-desc":"降序","listing-page-order-by-number-asc":"升序","listing-page-field-date":"日期","listing-page-field-title":"标题","listing-page-field-description":"描述","listing-page-field-author":"作者","listing-page-field-filename":"文件名","listing-page-field-filemodified":"修改时间","listing-page-field-subtitle":"副标题","listing-page-field-readingtime":"阅读时间","listing-page-field-wordcount":"字数统计","listing-page-field-categories":"分类","listing-page-minutes-compact":"{0} 分钟","listing-page-category-all":"全部","listing-page-no-matches":"无匹配项","listing-page-words":"{0} 字","listing-page-filter":"筛选","draft":"草稿"},"metadata":{"block-headings":true,"thesis-title":"现代计算机视觉","author":[{"name":"AI Assistant & You"}],"supervisor":[{"name":"Supervisor Name"}],"degree":"Doctor of Philosophy","degree-year":"2024","declaration-text":"This is my declaration.","acknowledgements-text":"I want to thank...","lang":"zh-CN","bibliography":["../references.bib"],"mermaid":{"theme":"default"},"documentclass":"scrreprt","mainfont":"PingFang SC"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}