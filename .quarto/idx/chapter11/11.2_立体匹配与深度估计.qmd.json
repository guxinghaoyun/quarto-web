{"title":"立体匹配与深度估计","markdown":{"headingText":"立体匹配与深度估计","containsRefs":false,"markdown":"\n## 引言：从双目视觉到深度感知\n\n当我们用双眼观察世界时，左右眼看到的图像存在微小差异。大脑正是利用这种差异来感知深度，判断物体的远近。立体匹配算法正是模拟了人类双目视觉的这一机制：通过计算两幅图像中对应点的视差（位置差异），恢复场景的深度信息。\n\n随着深度学习的发展，深度估计技术已经从传统的几何方法扩展到基于神经网络的端到端学习方法。现代深度估计系统不仅能处理标准的双目图像对，还能从单目图像直接预测深度，在自动驾驶、机器人导航、增强现实等领域发挥着关键作用。\n\n## 核心概念\n\n**传统立体匹配**基于几何约束和相似性度量。双目立体视觉系统通常由两个平行放置的相机组成，相机之间的距离称为基线（baseline）。传统方法通过在极线约束下搜索对应点，计算视差来恢复深度。这类方法计算效率高，但在弱纹理、遮挡区域容易失效。\n\n**深度学习方法**则将深度估计视为回归问题，通过端到端训练学习从图像到深度的映射关系。现代深度网络如PSMNet能够处理复杂场景，在准确性和鲁棒性方面显著超越传统方法。这类方法能够利用语义信息和全局上下文，在困难区域也能给出合理的深度估计。\n\n```{mermaid}\ngraph TD\n    subgraph 双目相机系统\n        A[\"左相机<br/>Camera_L\"]\n        B[\"右相机<br/>Camera_R\"]\n    end\n    \n    subgraph 图像获取\n        C[\"左图像<br/>Image_L\"]\n        D[\"右图像<br/>Image_R\"]\n    end\n    \n    subgraph 立体匹配\n        E[\"视差计算<br/>Disparity Map\"]\n    end\n    \n    subgraph 深度重建\n        F[\"深度图<br/>Depth Map\"]\n    end\n    \n    A --> C\n    B --> D\n    C --> E\n    D --> E\n    E --> F\n    \n    classDef cameraNode fill:#42a5f5,stroke:#1565c0,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    classDef imageNode fill:#66bb6a,stroke:#2e7d32,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    classDef disparityNode fill:#ffb74d,stroke:#e65100,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    classDef depthNode fill:#ef5350,stroke:#c62828,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    \n    classDef cameraSubgraph fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1,font-weight:bold\n    classDef imageSubgraph fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20,font-weight:bold\n    classDef disparitySubgraph fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#bf360c,font-weight:bold\n    classDef depthSubgraph fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#b71c1c,font-weight:bold\n    \n    class A,B cameraNode\n    class C,D imageNode\n    class E disparityNode\n    class F depthNode\n    \n    class 双目相机系统 cameraSubgraph\n    class 图像获取 imageSubgraph\n    class 立体匹配 disparitySubgraph\n    class 深度重建 depthSubgraph\n    \n    linkStyle 0,1 stroke:#1565c0,stroke-width:2px\n    linkStyle 2,3 stroke:#2e7d32,stroke-width:2px\n    linkStyle 4 stroke:#e65100,stroke-width:2px\n```\n*图11.6：双目立体视觉系统的基本工作流程*\n\n**视差（Disparity）**是立体匹配的核心概念。它指的是同一物体在左右图像中对应点的水平位置差异。视差与深度成反比关系：距离相机越近的物体，其视差越大；距离相机越远的物体，其视差越小。无穷远处的物体（如天空）视差接近于零。\n\n**对应点问题**是立体匹配的核心挑战。给定左图中的一个点，如何在右图中找到与之对应的点？这个看似简单的问题实际上非常复杂，尤其是在纹理缺乏、重复模式、遮挡区域等情况下。立体匹配算法的主要差异就在于如何解决这个对应点问题。\n\n```{mermaid}\ngraph LR\n    subgraph 左图像\n        A[\"参考点<br/>(x, y)\"]\n    end\n    \n    subgraph 右图像\n        B[\"匹配点<br/>(x-d, y)\"]\n        C[\"非匹配点\"]\n    end\n    \n    A -->|\"匹配搜索\"| B\n    A -.->|\"错误匹配\"| C\n    \n    subgraph 匹配约束\n        D[\"极线约束\"]\n        E[\"唯一性约束\"]\n        F[\"顺序一致性约束\"]\n        G[\"视差平滑约束\"]\n    end\n    \n    D --> A\n    E --> A\n    F --> A\n    G --> A\n    \n    classDef leftNode fill:#42a5f5,stroke:#1565c0,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    classDef rightNode fill:#66bb6a,stroke:#2e7d32,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    classDef wrongNode fill:#ef5350,stroke:#c62828,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    classDef constraintNode fill:#ba68c8,stroke:#7b1fa2,color:white,stroke-width:2px,font-weight:bold,font-size:14px,border-radius:8px\n    \n    classDef leftSubgraph fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1,font-weight:bold\n    classDef rightSubgraph fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20,font-weight:bold\n    classDef constraintSubgraph fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c,font-weight:bold\n    \n    class A leftNode\n    class B rightNode\n    class C wrongNode\n    class D,E,F,G constraintNode\n    \n    class 左图像 leftSubgraph\n    class 右图像 rightSubgraph\n    class 匹配约束 constraintSubgraph\n    \n    linkStyle 0 stroke:#4caf50,stroke-width:2px\n    linkStyle 1 stroke:#f44336,stroke-width:2px,stroke-dasharray:5 5\n    linkStyle 2,3,4,5 stroke:#9c27b0,stroke-width:1.5px\n```\n*图11.7：立体匹配中的对应点问题与匹配约束*\n\n## 理论基础：从几何约束到深度学习\n\n立体匹配与深度估计的理论基础可以分为传统几何方法和现代深度学习方法两大类。\n\n### 传统几何方法的理论基础\n\n**1. 立体相机几何关系**\n\n在标准立体配置中，两个相机的光轴平行，图像平面共面。设左右相机的光心分别为$O_L$和$O_R$，它们之间的距离（基线长度）为$b$。对于空间中的点$P(X,Y,Z)$，其在左右图像中的投影点分别为$p_L(x_L,y_L)$和$p_R(x_R,y_R)$。根据相似三角形原理：\n\n$$\\frac{x_L - x_R}{b} = \\frac{f}{Z}$$\n\n定义视差$d = x_L - x_R$，则深度与视差成反比关系：\n\n$$Z = \\frac{f \\cdot b}{d}$$\n\n**2. 传统视差计算方法**\n\n传统方法主要基于匹配代价计算和优化：\n\n- **局部方法**：使用窗口匹配，计算相似性度量如SAD、SSD或NCC：\n\n$$\\text{SAD}(x,y,d) = \\sum_{(i,j) \\in W} |I_L(i,j) - I_R(i-d,j)|$$\n\n- **全局方法**：将视差计算视为能量最小化问题：\n\n$$E(D) = E_{data}(D) + \\lambda \\cdot E_{smooth}(D)$$\n\n- **半全局方法(SGM)**：通过多方向路径聚合匹配代价，平衡局部和全局信息：\n\n$$L_r(p,d) = C(p,d) + \\min \\begin{cases}\nL_r(p-r,d) \\\\\nL_r(p-r,d-1) + P_1 \\\\\nL_r(p-r,d+1) + P_1 \\\\\n\\min_i L_r(p-r,i) + P_2\n\\end{cases}$$\n\n其中$L_r(p,d)$是沿方向$r$的路径代价，$C(p,d)$是像素$p$处视差为$d$的匹配代价，$P_1$和$P_2$是平滑性惩罚参数。\n\n### 深度学习方法的理论基础\n\n**1. 端到端深度估计框架**\n\n深度学习方法将立体匹配视为一个端到端的回归问题，网络架构通常包含四个关键组件：\n\n- **特征提取**：使用CNN提取左右图像的特征表示\n- **代价体积构建**：通过特征匹配或拼接构建4D代价体积\n- **代价聚合**：使用3D CNN或GNN进行代价聚合\n- **视差回归**：通过软argmin操作回归连续视差值\n\n**2. PSMNet的核心理论**\n\nPSMNet是深度学习立体匹配的代表性网络，其核心理论包括：\n\n- **空间金字塔池化(SPP)**：捕获多尺度上下文信息：\n\n$$F_{SPP}(x) = \\text{Concat}[F(x), P_1(F(x)), P_2(F(x)), ..., P_n(F(x))]$$\n\n其中$P_i$表示不同尺度的池化操作。\n\n- **3D代价体积滤波**：使用3D CNN进行代价聚合：\n\n$$C_{out} = \\text{3DCNN}(C_{in})$$\n\n- **视差回归**：通过软argmin操作实现亚像素精度：\n\n$$\\hat{d} = \\sum_{d=0}^{D_{max}} d \\cdot \\sigma(-C_d)$$\n\n其中$\\sigma$是softmax函数，$C_d$是代价体积中视差为$d$的代价值。\n\n**3. 单目深度估计理论**\n\n单目深度估计直接从单张图像预测深度，其理论基础是：\n\n- **编码器-解码器架构**：通过多尺度特征提取和逐步上采样恢复分辨率\n- **深度回归**：直接回归深度值或视差值\n- **自监督学习**：利用时序一致性或立体一致性作为监督信号：\n\n$$L_{photo} = \\alpha \\frac{1-\\text{SSIM}(I, \\hat{I})}{2} + (1-\\alpha)||I-\\hat{I}||_1$$\n\n其中$\\hat{I}$是通过预测的深度图和相机位姿重投影得到的图像。\n\n这些理论方法的核心区别在于：传统方法依赖手工设计的特征和几何约束，而深度学习方法能够自动学习特征表示和匹配策略，特别是在复杂场景中表现出更强的鲁棒性。\n\n## 算法实现\n\n立体匹配与深度估计的实现可以分为传统几何方法和现代深度学习方法两大类。\n\n**传统SGBM算法核心**：\n\n```python\nimport cv2\nimport numpy as np\n\ndef sgbm_stereo_matching(left_img, right_img):\n    \"\"\"\n    SGBM立体匹配核心算法\n    核心思想：通过多方向路径聚合优化匹配代价\n    \"\"\"\n    # 核心参数设置\n    stereo = cv2.StereoSGBM_create(\n        minDisparity=0,\n        numDisparities=64,          # 视差搜索范围\n        blockSize=5,                # 匹配窗口大小\n        P1=8 * 3 * 5**2,           # 平滑性惩罚参数1\n        P2=32 * 3 * 5**2,          # 平滑性惩罚参数2\n        uniquenessRatio=10,         # 唯一性比率\n        speckleWindowSize=100,      # 斑点滤波窗口\n        speckleRange=32             # 斑点滤波范围\n    )\n\n    # 计算视差图\n    disparity = stereo.compute(left_img, right_img)\n    return disparity.astype(np.float32) / 16.0  # 转换为真实视差值\n\ndef disparity_to_depth(disparity, focal_length, baseline):\n    \"\"\"视差转深度的核心公式\"\"\"\n    return (focal_length * baseline) / (disparity + 1e-6)\n```\n\n**现代PSMNet深度网络**：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PSMNet(nn.Module):\n    \"\"\"\n    PSMNet核心架构\n    核心思想：构建4D代价体积，通过3D CNN进行代价聚合\n    \"\"\"\n    def __init__(self, maxdisp=192):\n        super(PSMNet, self).__init__()\n        self.maxdisp = maxdisp\n\n        # 特征提取网络\n        self.feature_extraction = self._make_feature_extractor()\n\n        # 代价体积构建\n        self.cost_volume_filter = self._make_cost_volume_filter()\n\n        # 视差回归\n        self.disparity_regression = self._make_disparity_regression()\n\n    def forward(self, left, right):\n        # 1. 特征提取\n        left_features = self.feature_extraction(left)\n        right_features = self.feature_extraction(right)\n\n        # 2. 构建代价体积\n        cost_volume = self.build_cost_volume(left_features, right_features)\n\n        # 3. 代价聚合\n        cost_volume = self.cost_volume_filter(cost_volume)\n\n        # 4. 视差回归\n        disparity = self.disparity_regression(cost_volume)\n\n        return disparity\n\n    def build_cost_volume(self, left_feat, right_feat):\n        \"\"\"构建4D代价体积的核心逻辑\"\"\"\n        B, C, H, W = left_feat.shape\n        cost_volume = torch.zeros(B, C*2, self.maxdisp//4, H, W)\n\n        for i in range(self.maxdisp//4):\n            if i > 0:\n                cost_volume[:, :C, i, :, i:] = left_feat[:, :, :, i:]\n                cost_volume[:, C:, i, :, i:] = right_feat[:, :, :, :-i]\n            else:\n                cost_volume[:, :C, i, :, :] = left_feat\n                cost_volume[:, C:, i, :, :] = right_feat\n\n        return cost_volume\n```\n\n**单目深度估计核心**：\n\n```python\nclass MonoDepthNet(nn.Module):\n    \"\"\"\n    单目深度估计网络核心\n    核心思想：从单张图像直接回归深度图\n    \"\"\"\n    def __init__(self):\n        super(MonoDepthNet, self).__init__()\n        # 编码器：提取多尺度特征\n        self.encoder = self._make_encoder()\n        # 解码器：逐步上采样恢复分辨率\n        self.decoder = self._make_decoder()\n\n    def forward(self, x):\n        # 多尺度特征提取\n        features = self.encoder(x)\n        # 深度图回归\n        depth = self.decoder(features)\n        return depth\n```\n\n这些算法的核心区别在于：SGBM基于几何约束和手工特征，PSMNet通过学习特征和代价聚合，单目方法则完全依赖语义理解。现代方法在复杂场景下表现更佳，但计算成本也更高。\n\n```{mermaid}\nflowchart TD\n    A[\"输入立体图像对\"] --> B[\"图像预处理<br/>灰度转换、滤波\"]\n    B --> C[\"特征提取<br/>梯度、Census变换等\"]\n    C --> D[\"代价计算<br/>SAD/SSD/Census等\"]\n    D --> E[\"代价聚合<br/>窗口聚合/路径聚合\"]\n    E --> F[\"视差优化<br/>赢家通吃/动态规划\"]\n    F --> G[\"视差细化<br/>亚像素插值、滤波\"]\n    G --> H[\"深度转换<br/>Z = f·b/d\"]\n\n    subgraph 预处理阶段\n        A\n        B\n    end\n\n    subgraph 匹配代价阶段\n        C\n        D\n    end\n\n    subgraph 优化阶段\n        E\n        F\n        G\n    end\n\n    subgraph 后处理阶段\n        H\n    end\n\n    classDef prepNode fill:#42a5f5,stroke:#1565c0,color:white,stroke-width:2px,font-weight:bold,border-radius:8px\n    classDef costNode fill:#ffb74d,stroke:#e65100,color:white,stroke-width:2px,font-weight:bold,border-radius:8px\n    classDef optNode fill:#66bb6a,stroke:#2e7d32,color:white,stroke-width:2px,font-weight:bold,border-radius:8px\n    classDef postNode fill:#ef5350,stroke:#c62828,color:white,stroke-width:2px,font-weight:bold,border-radius:8px\n\n    classDef prepSubgraph fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1,font-weight:bold\n    classDef costSubgraph fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#bf360c,font-weight:bold\n    classDef optSubgraph fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20,font-weight:bold\n    classDef postSubgraph fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#b71c1c,font-weight:bold\n\n    class A,B prepNode\n    class C,D costNode\n    class E,F,G optNode\n    class H postNode\n\n    class 预处理阶段 prepSubgraph\n    class 匹配代价阶段 costSubgraph\n    class 优化阶段 optSubgraph\n    class 后处理阶段 postSubgraph\n\n    linkStyle 0,1,2,3,4,5,6 stroke-width:2px\n```\n*图11.8：立体匹配算法的通用流程*\n\n## 性能对比分析\n\n深度估计算法的效果可以通过视差图和深度图的质量来评估。下面我们分析传统方法和深度学习方法在不同场景下的表现。\n\n**算法性能对比**：\n\n```{mermaid}\ngraph TD\n    subgraph 传统方法\n        A[\"块匹配(BM)<br/>速度: 快<br/>精度: 低<br/>内存: 低\"]\n        B[\"半全局匹配(SGBM)<br/>速度: 中<br/>精度: 中<br/>内存: 低\"]\n        C[\"全局匹配(GC/BP)<br/>速度: 慢<br/>精度: 高<br/>内存: 中\"]\n    end\n\n    subgraph 深度学习方法\n        D[\"PSMNet<br/>速度: 慢<br/>精度: 很高<br/>内存: 高\"]\n        E[\"GANet<br/>速度: 很慢<br/>精度: 最高<br/>内存: 很高\"]\n        F[\"单目深度估计<br/>速度: 中<br/>精度: 中<br/>内存: 中\"]\n    end\n\n    subgraph 性能指标\n        G[\"KITTI 3px错误率\"]\n        H[\"Middlebury平均误差\"]\n        I[\"ETH3D完整性\"]\n    end\n\n    A --> G\n    B --> G\n    C --> G\n    D --> G\n    E --> G\n    F --> G\n\n    G --> J[\"传统: 5-15%<br/>深度学习: 2-5%\"]\n    H --> K[\"传统: 1-3px<br/>深度学习: 0.5-1px\"]\n    I --> L[\"传统: 70-90%<br/>深度学习: 90-98%\"]\n\n    classDef tradNode fill:#64b5f6,stroke:#1565c0,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef dlNode fill:#ba68c8,stroke:#7b1fa2,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef metricNode fill:#66bb6a,stroke:#2e7d32,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef resultNode fill:#ffb74d,stroke:#e65100,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n\n    classDef tradSubgraph fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1,font-weight:bold\n    classDef dlSubgraph fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c,font-weight:bold\n    classDef metricSubgraph fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20,font-weight:bold\n\n    class A,B,C tradNode\n    class D,E,F dlNode\n    class G,H,I metricNode\n    class J,K,L resultNode\n\n    class 传统方法 tradSubgraph\n    class 深度学习方法 dlSubgraph\n    class 性能指标 metricSubgraph\n\n    linkStyle 0,1,2,3,4,5 stroke:#1565c0,stroke-width:1.5px\n    linkStyle 6,7,8 stroke:#4caf50,stroke-width:1.5px\n```\n*图11.9：传统方法与深度学习方法的性能对比*\n\n**场景适应性分析**：\n\n```{mermaid}\ngraph TD\n    subgraph 场景特征\n        A[\"纹理丰富<br/>结构清晰\"]\n        B[\"弱纹理区域<br/>重复模式\"]\n        C[\"反光表面<br/>透明物体\"]\n        D[\"遮挡区域<br/>边界不连续\"]\n    end\n\n    subgraph 传统方法表现\n        E[\"SGBM<br/>准确度: 高<br/>鲁棒性: 中\"]\n        F[\"BM<br/>准确度: 中<br/>鲁棒性: 低\"]\n        G[\"GC<br/>准确度: 高<br/>鲁棒性: 中\"]\n    end\n\n    subgraph 深度学习方法表现\n        H[\"PSMNet<br/>准确度: 很高<br/>鲁棒性: 高\"]\n        I[\"GANet<br/>准确度: 最高<br/>鲁棒性: 很高\"]\n        J[\"单目深度<br/>准确度: 中<br/>鲁棒性: 高\"]\n    end\n\n    A --> E\n    A --> H\n    B --> G\n    B --> I\n    C --> F\n    C --> J\n    D --> G\n    D --> I\n\n    classDef sceneNode fill:#4db6ac,stroke:#00796b,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef tradNode fill:#64b5f6,stroke:#1565c0,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef dlNode fill:#ba68c8,stroke:#7b1fa2,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n\n    classDef sceneSubgraph fill:#e0f2f1,stroke:#00796b,stroke-width:2px,color:#004d40,font-weight:bold\n    classDef tradSubgraph fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1,font-weight:bold\n    classDef dlSubgraph fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c,font-weight:bold\n\n    class A,B,C,D sceneNode\n    class E,F,G tradNode\n    class H,I,J dlNode\n\n    class 场景特征 sceneSubgraph\n    class 传统方法表现 tradSubgraph\n    class 深度学习方法表现 dlSubgraph\n\n    linkStyle 0,1,2,3,4,5,6,7 stroke-width:1.5px\n```\n*图11.10：不同方法在各类场景中的适应性分析*\n\n**深度学习方法的进展**：\n\n```{mermaid}\ngraph LR\n    subgraph 网络架构演进\n        A[\"2D CNN<br/>(DispNet, 2016)\"]\n        B[\"3D CNN<br/>(PSMNet, 2018)\"]\n        C[\"GNN<br/>(GwcNet, 2019)\"]\n        D[\"Transformer<br/>(STTR, 2021)\"]\n    end\n\n    subgraph 关键技术创新\n        E[\"代价体积构建\"]\n        F[\"多尺度特征融合\"]\n        G[\"注意力机制\"]\n        H[\"自监督学习\"]\n    end\n\n    A --> B\n    B --> C\n    C --> D\n\n    A --> E\n    B --> F\n    C --> G\n    D --> H\n\n    classDef archNode fill:#ef5350,stroke:#c62828,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n    classDef techNode fill:#66bb6a,stroke:#2e7d32,color:white,stroke-width:2px,font-weight:bold,font-size:13px,border-radius:8px\n\n    classDef archSubgraph fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#b71c1c,font-weight:bold\n    classDef techSubgraph fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20,font-weight:bold\n\n    class A,B,C,D archNode\n    class E,F,G,H techNode\n\n    class 网络架构演进 archSubgraph\n    class 关键技术创新 techSubgraph\n\n    linkStyle 0,1,2 stroke:#f44336,stroke-width:1.5px\n    linkStyle 3,4,5,6 stroke:#4caf50,stroke-width:1.5px\n```\n*图11.11：深度学习立体匹配方法的技术演进*\n\n## 小结\n\n立体匹配与深度估计是三维视觉的核心技术，经历了从传统几何方法到深度学习方法的重要演进。传统方法如SGBM基于几何约束和手工特征，计算效率高但在复杂场景下容易失效。现代深度学习方法如PSMNet通过端到端学习，在准确性和鲁棒性方面显著超越传统方法。\n\n本节的核心贡献在于：**理论层面**，阐述了从视差计算到深度回归的算法演进逻辑；**技术层面**，对比了传统方法和深度学习方法的核心差异；**应用层面**，分析了不同方法在各类场景中的适应性。\n\n深度估计技术与相机标定紧密相连：准确的相机标定是高质量深度估计的前提。同时，深度估计也为后续的三维重建和点云处理提供了基础数据。随着Transformer等新架构的引入，深度估计正朝着更高精度、更强泛化能力的方向发展，在自动驾驶、机器人等领域发挥着越来越重要的作用。\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"atom-one","output-file":"11.2_立体匹配与深度估计.html"},"language":{"toc-title-document":"目录","toc-title-website":"该页面内容","related-formats-title":"其他格式","related-notebooks-title":"笔记本","source-notebooks-prefix":"资源","other-links-title":"其他链接","code-links-title":"代码链接","launch-dev-container-title":"启动 Dev Container","launch-binder-title":"启动 Binder","article-notebook-label":"文章笔记本","notebook-preview-download":"下载笔记本","notebook-preview-download-src":"下载源代码","notebook-preview-back":"返回文章","manuscript-meca-bundle":"MECA 存档","section-title-abstract":"摘要","section-title-appendices":"附录","section-title-footnotes":"脚注","section-title-references":"参考文献","section-title-reuse":"二次使用","section-title-copyright":"版权","section-title-citation":"引用格式","appendix-attribution-cite-as":"请按如下格式引用：","appendix-attribution-bibtex":"BibTeX","appendix-view-license":"查看许可协议","title-block-author-single":"作者","title-block-author-plural":"作者","title-block-affiliation-single":"单位","title-block-affiliation-plural":"单位","title-block-published":"发布于","title-block-modified":"修改于","title-block-keywords":"关键词","callout-tip-title":"提示","callout-note-title":"注记","callout-warning-title":"警告","callout-important-title":"重要","callout-caution-title":"注意","code-summary":"代码","code-tools-menu-caption":"代码","code-tools-show-all-code":"显示所有代码","code-tools-hide-all-code":"隐藏所有代码","code-tools-view-source":"查看源代码","code-tools-source-code":"源代码","tools-share":"Share","tools-download":"Download","code-line":"行","code-lines":"行","copy-button-tooltip":"复制到剪贴板","copy-button-tooltip-success":"已复制","repo-action-links-edit":"编辑该页面","repo-action-links-source":"查看代码","repo-action-links-issue":"反馈问题","back-to-top":"回到顶部","search-no-results-text":"没有结果","search-matching-documents-text":"匹配的文档","search-copy-link-title":"复制搜索链接","search-hide-matches-text":"隐藏其它匹配结果","search-more-match-text":"更多匹配结果","search-more-matches-text":"更多匹配结果","search-clear-button-title":"清除","search-text-placeholder":"","search-detached-cancel-button-title":"取消","search-submit-button-title":"提交","search-label":"搜索","toggle-section":"展开或折叠此栏","toggle-sidebar":"展开或折叠侧边栏导航","toggle-dark-mode":"切换深色模式","toggle-reader-mode":"切换阅读器模式","toggle-navigation":"展开或折叠导航栏","crossref-fig-title":"图","crossref-tbl-title":"表","crossref-lst-title":"列表","crossref-thm-title":"定理","crossref-lem-title":"引理","crossref-cor-title":"推论","crossref-prp-title":"命题","crossref-cnj-title":"猜想","crossref-def-title":"定义","crossref-exm-title":"例","crossref-exr-title":"习题","crossref-ch-prefix":"章节","crossref-apx-prefix":"附录","crossref-sec-prefix":"小节","crossref-eq-prefix":"式","crossref-lof-title":"图索引","crossref-lot-title":"表索引","crossref-lol-title":"列表索引","environment-proof-title":"证","environment-remark-title":"注记","environment-solution-title":"解","listing-page-order-by":"排序方式","listing-page-order-by-default":"默认","listing-page-order-by-date-asc":"日期升序","listing-page-order-by-date-desc":"日期降序","listing-page-order-by-number-desc":"降序","listing-page-order-by-number-asc":"升序","listing-page-field-date":"日期","listing-page-field-title":"标题","listing-page-field-description":"描述","listing-page-field-author":"作者","listing-page-field-filename":"文件名","listing-page-field-filemodified":"修改时间","listing-page-field-subtitle":"副标题","listing-page-field-readingtime":"阅读时间","listing-page-field-wordcount":"字数统计","listing-page-field-categories":"分类","listing-page-minutes-compact":"{0} 分钟","listing-page-category-all":"全部","listing-page-no-matches":"无匹配项","listing-page-words":"{0} 字","listing-page-filter":"筛选","draft":"草稿"},"metadata":{"lang":"zh-CN","fig-responsive":true,"quarto-version":"1.7.32","thesis-title":"现代计算机视觉","author":[{"name":"AI Assistant & You"}],"supervisor":[{"name":"Supervisor Name"}],"degree":"Doctor of Philosophy","degree-year":"2025","declaration-text":"This is my declaration.","acknowledgements-text":"I want to thank...","bibliography":["../references.bib"],"mermaid":{"theme":"default"},"theme":{"light":"flatly","dark":"cyborg"},"mainfont":"WenQuanYi Micro Hei"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"mermaid-format":"png","df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"11.2_立体匹配与深度估计.pdf"},"language":{"toc-title-document":"目录","toc-title-website":"该页面内容","related-formats-title":"其他格式","related-notebooks-title":"笔记本","source-notebooks-prefix":"资源","other-links-title":"其他链接","code-links-title":"代码链接","launch-dev-container-title":"启动 Dev Container","launch-binder-title":"启动 Binder","article-notebook-label":"文章笔记本","notebook-preview-download":"下载笔记本","notebook-preview-download-src":"下载源代码","notebook-preview-back":"返回文章","manuscript-meca-bundle":"MECA 存档","section-title-abstract":"摘要","section-title-appendices":"附录","section-title-footnotes":"脚注","section-title-references":"参考文献","section-title-reuse":"二次使用","section-title-copyright":"版权","section-title-citation":"引用格式","appendix-attribution-cite-as":"请按如下格式引用：","appendix-attribution-bibtex":"BibTeX","appendix-view-license":"查看许可协议","title-block-author-single":"作者","title-block-author-plural":"作者","title-block-affiliation-single":"单位","title-block-affiliation-plural":"单位","title-block-published":"发布于","title-block-modified":"修改于","title-block-keywords":"关键词","callout-tip-title":"提示","callout-note-title":"注记","callout-warning-title":"警告","callout-important-title":"重要","callout-caution-title":"注意","code-summary":"代码","code-tools-menu-caption":"代码","code-tools-show-all-code":"显示所有代码","code-tools-hide-all-code":"隐藏所有代码","code-tools-view-source":"查看源代码","code-tools-source-code":"源代码","tools-share":"Share","tools-download":"Download","code-line":"行","code-lines":"行","copy-button-tooltip":"复制到剪贴板","copy-button-tooltip-success":"已复制","repo-action-links-edit":"编辑该页面","repo-action-links-source":"查看代码","repo-action-links-issue":"反馈问题","back-to-top":"回到顶部","search-no-results-text":"没有结果","search-matching-documents-text":"匹配的文档","search-copy-link-title":"复制搜索链接","search-hide-matches-text":"隐藏其它匹配结果","search-more-match-text":"更多匹配结果","search-more-matches-text":"更多匹配结果","search-clear-button-title":"清除","search-text-placeholder":"","search-detached-cancel-button-title":"取消","search-submit-button-title":"提交","search-label":"搜索","toggle-section":"展开或折叠此栏","toggle-sidebar":"展开或折叠侧边栏导航","toggle-dark-mode":"切换深色模式","toggle-reader-mode":"切换阅读器模式","toggle-navigation":"展开或折叠导航栏","crossref-fig-title":"图","crossref-tbl-title":"表","crossref-lst-title":"列表","crossref-thm-title":"定理","crossref-lem-title":"引理","crossref-cor-title":"推论","crossref-prp-title":"命题","crossref-cnj-title":"猜想","crossref-def-title":"定义","crossref-exm-title":"例","crossref-exr-title":"习题","crossref-ch-prefix":"章节","crossref-apx-prefix":"附录","crossref-sec-prefix":"小节","crossref-eq-prefix":"式","crossref-lof-title":"图索引","crossref-lot-title":"表索引","crossref-lol-title":"列表索引","environment-proof-title":"证","environment-remark-title":"注记","environment-solution-title":"解","listing-page-order-by":"排序方式","listing-page-order-by-default":"默认","listing-page-order-by-date-asc":"日期升序","listing-page-order-by-date-desc":"日期降序","listing-page-order-by-number-desc":"降序","listing-page-order-by-number-asc":"升序","listing-page-field-date":"日期","listing-page-field-title":"标题","listing-page-field-description":"描述","listing-page-field-author":"作者","listing-page-field-filename":"文件名","listing-page-field-filemodified":"修改时间","listing-page-field-subtitle":"副标题","listing-page-field-readingtime":"阅读时间","listing-page-field-wordcount":"字数统计","listing-page-field-categories":"分类","listing-page-minutes-compact":"{0} 分钟","listing-page-category-all":"全部","listing-page-no-matches":"无匹配项","listing-page-words":"{0} 字","listing-page-filter":"筛选","draft":"草稿"},"metadata":{"block-headings":true,"thesis-title":"现代计算机视觉","author":[{"name":"AI Assistant & You"}],"supervisor":[{"name":"Supervisor Name"}],"degree":"Doctor of Philosophy","degree-year":"2025","declaration-text":"This is my declaration.","acknowledgements-text":"I want to thank...","lang":"zh-CN","bibliography":["../references.bib"],"mermaid":{"theme":"default"},"documentclass":"scrreprt","mainfont":"PingFang SC"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}